---
title: "Speech Translation & Dubbing"
description: "Translate spoken content between languages while preserving voice characteristics, timing, and emotional expression using AudioPod AI's advanced dubbing technology."
---

## Overview

AudioPod AI's Speech Translation & Dubbing service converts spoken content from one language to another while preserving the original speaker's voice characteristics, timing, and emotional expression. Perfect for localizing videos, creating multilingual content, and breaking language barriers.

### Key Features

- **Voice-Preserving Translation**: Maintain speaker's voice while changing language
- **64+ Language Pairs**: Support for major language combinations
- **Automatic Timing Sync**: Preserve original video timing and lip-sync
- **Emotion Preservation**: Keep emotional expression across languages
- **Custom Voice Mapping**: Use specific voices for different speakers
- **Batch Processing**: Process multiple files or long-form content
- **Real-time Translation**: Live dubbing for streaming applications

## Quick Start

### Basic Speech Translation

<Tabs>
  <Tab title="Python">
    ```python
    from audiopod import AudioPod

    client = AudioPod(api_key="your_api_key")

    # Translate speech to another language
    with open("english_speech.mp3", "rb") as audio_file:
        response = client.speech_translation.translate(
            audio=audio_file,
            source_language="en",
            target_language="es",
            preserve_voice=True,
            sync_timing=True
        )

    # Save translated audio
    with open("spanish_speech.mp3", "wb") as f:
        f.write(response.audio_data)

    print(f"Translation completed!")
    print(f"Original: {response.original_text}")
    print(f"Translated: {response.translated_text}")
    print(f"Duration: {response.duration}s")
    ```
  </Tab>
  <Tab title="Node.js">
    ```javascript
    import { AudioPod } from 'audiopod-js';
    import fs from 'fs';

    const client = new AudioPod({ apiKey: 'your_api_key' });

    const audioFile = fs.readFileSync('english_speech.mp3');

    const response = await client.speechTranslation.translate({
      audio: audioFile,
      sourceLanguage: 'en',
      targetLanguage: 'es',
      preserveVoice: true,
      syncTiming: true
    });

    fs.writeFileSync('spanish_speech.mp3', response.audioData);

    console.log('Translation completed!');
    console.log(`Original: ${response.originalText}`);
    console.log(`Translated: ${response.translatedText}`);
    console.log(`Duration: ${response.duration}s`);
    ```
  </Tab>
  <Tab title="cURL">
    ```bash
    curl -X POST "https://api.audiopod.ai/api/v1/speech-translation" \
      -H "Authorization: Bearer your_api_key" \
      -F "audio=@english_speech.mp3" \
      -F "source_language=en" \
      -F "target_language=es" \
      -F "preserve_voice=true" \
      -F "sync_timing=true" \
      --output spanish_speech.mp3
    ```
  </Tab>
</Tabs>

### Response Format

```json
{
  "audio_data": "base64_encoded_audio",
  "original_text": "Hello everyone, welcome to our presentation about artificial intelligence.",
  "translated_text": "Hola a todos, bienvenidos a nuestra presentación sobre inteligencia artificial.",
  "source_language": "en",
  "target_language": "es",
  "duration": 8.5,
  "voice_characteristics": {
    "preserved": true,
    "similarity_score": 0.89,
    "gender": "female",
    "age_estimate": "25-35"
  },
  "timing_info": {
    "original_duration": 8.2,
    "translated_duration": 8.5,
    "sync_quality": 0.94,
    "segments": [
      {
        "original": "Hello everyone,",
        "translated": "Hola a todos,",
        "start_time": 0.0,
        "end_time": 1.8
      }
    ]
  }
}
```

## Supported Languages

### Language Pairs

AudioPod AI supports translation between 64+ languages with varying quality levels:

<Tabs>
  <Tab title="Tier 1 (Excellent Quality)">
    **From/To English:**
    - Spanish (es) - Mexican, European, South American
    - French (fr) - Metropolitan, Canadian 
    - German (de) - Standard German
    - Italian (it) - Standard Italian
    - Portuguese (pt) - Brazilian, European
    - Dutch (nl) - Netherlands Dutch
    - Swedish (sv) - Standard Swedish
    - Norwegian (no) - Bokmål Norwegian
    
    **Popular Cross-Language Pairs:**
    - Spanish ↔ French
    - German ↔ French  
    - Italian ↔ Spanish
    - Portuguese ↔ Spanish
  </Tab>
  <Tab title="Tier 2 (Very Good Quality)">
    **From/To English:**
    - Chinese (zh) - Mandarin, Simplified/Traditional
    - Japanese (ja) - Standard Japanese
    - Korean (ko) - Standard Korean
    - Russian (ru) - Standard Russian
    - Arabic (ar) - Modern Standard Arabic
    - Hindi (hi) - Standard Hindi
    - Polish (pl) - Standard Polish
    - Turkish (tr) - Standard Turkish
    
    **Cross-Language Support:**
    - Chinese ↔ Japanese
    - Korean ↔ Japanese
    - Russian ↔ German
    - Arabic ↔ French
  </Tab>
  <Tab title="Tier 3 (Good Quality)">
    **Additional Languages:**
    - Finnish (fi), Danish (da), Czech (cs)
    - Hungarian (hu), Romanian (ro), Greek (el)
    - Thai (th), Vietnamese (vi), Indonesian (id)
    - Hebrew (he), Ukrainian (uk), Slovak (sk)
    - Croatian (hr), Bulgarian (bg), Lithuanian (lt)
    
    **Note:** Best results when translating through English
    (Source → English → Target)
  </Tab>
</Tabs>

### Auto Language Detection

Automatically detect the source language:

```python
response = client.speech_translation.translate(
    audio=audio_file,
    source_language="auto",  # Automatic detection
    target_language="fr",
    confidence_threshold=0.8  # Minimum detection confidence
)

print(f"Detected language: {response.detected_language}")
print(f"Detection confidence: {response.detection_confidence}")
```

## Advanced Features

### Video Dubbing with Timing Sync

Synchronize translated audio with video content:

```python
# Dub a video file with precise timing
response = client.speech_translation.dub_video(
    video_file="original_video.mp4",
    source_language="en",
    target_language="es",
    
    # Timing synchronization
    preserve_timing=True,        # Match original timing
    adjust_speed=True,           # Slightly adjust speech speed if needed
    max_speed_change=1.15,       # Maximum 15% speed adjustment
    
    # Voice and quality
    preserve_voice=True,         # Keep original voice characteristics
    voice_clone_quality="high",  # Voice preservation quality
    
    # Advanced options
    detect_speakers=True,        # Handle multiple speakers
    preserve_music=True,         # Keep background music
    noise_reduction=True         # Clean up audio
)

# Save dubbed video
with open("dubbed_video.mp4", "wb") as f:
    f.write(response.video_data)
```

### Multi-Speaker Translation

Handle multiple speakers with different voices:

```python
# Process content with multiple speakers
response = client.speech_translation.translate(
    audio=audio_file,
    source_language="en",
    target_language="fr",
    
    # Speaker handling
    enable_speaker_diarization=True,
    max_speakers=3,
    speaker_mapping={
        "Speaker_1": "male_professional",
        "Speaker_2": "female_warm", 
        "Speaker_3": "young_male"
    },
    
    # Voice preservation per speaker
    preserve_individual_voices=True,
    voice_consistency=True
)

# Access per-speaker translations
for segment in response.speaker_segments:
    print(f"{segment.speaker}: {segment.translated_text}")
    print(f"Voice similarity: {segment.voice_similarity}")
```

### Custom Voice Assignment

Use specific voices for translation:

```python
# Use custom voices instead of voice preservation
response = client.speech_translation.translate(
    audio=audio_file,
    source_language="en", 
    target_language="ja",
    
    # Custom voice assignment
    target_voice="yuki-jp",      # Japanese female voice
    voice_style="professional",   # Speaking style
    preserve_emotion=True,       # Keep emotional expression
    
    # Alternative: Use voice clones
    use_voice_clone="custom_voice_id",
    clone_adaptation="high"      # Adapt clone to target language
)
```

### Real-time Dubbing

Live translation for streaming applications:

```python
# Initialize real-time dubbing stream
dubbing_stream = client.speech_translation.create_stream(
    source_language="en",
    target_language="es",
    latency="low",              # low, medium, high
    buffer_duration=3.0,        # 3-second processing buffer
    preserve_voice=True,
    real_time_sync=True
)

# Process audio stream
def process_audio_stream():
    while streaming:
        # Get audio chunk from microphone/stream
        audio_chunk = get_audio_input()
        
        # Translate in real-time
        result = dubbing_stream.process(audio_chunk)
        
        if result.is_complete:
            # Play translated audio
            play_audio(result.translated_audio)
            print(f"Translated: {result.translated_text}")

# Start streaming
process_audio_stream()
dubbing_stream.close()
```

## Use Cases & Examples

### Video Content Localization

```python
def localize_video_content(video_file, target_languages, speaker_info=None):
    """Localize video content for multiple markets"""
    
    localized_versions = {}
    
    for lang_code in target_languages:
        print(f"Creating {lang_code} version...")
        
        # Configure based on target market
        market_config = get_market_config(lang_code)
        
        dubbed_video = client.speech_translation.dub_video(
            video_file=video_file,
            source_language="en",
            target_language=lang_code,
            
            # Market-specific settings
            cultural_adaptation=market_config["cultural_adaptation"],
            voice_preference=market_config["voice_style"],
            formality_level=market_config["formality"],
            
            # Technical settings
            preserve_timing=True,
            quality="premium",
            preserve_music=True,
            subtitle_sync=True  # Generate synced subtitles
        )
        
        localized_versions[lang_code] = {
            "video": dubbed_video.video_data,
            "subtitles": dubbed_video.subtitles,
            "voice_info": dubbed_video.voice_characteristics,
            "quality_metrics": dubbed_video.quality_score
        }
    
    return localized_versions

def get_market_config(language_code):
    """Get market-specific dubbing preferences"""
    configs = {
        "es": {  # Spanish
            "cultural_adaptation": True,
            "voice_style": "warm_latino",
            "formality": "medium"
        },
        "fr": {  # French
            "cultural_adaptation": True,
            "voice_style": "elegant_european",
            "formality": "formal"
        },
        "de": {  # German
            "cultural_adaptation": False,
            "voice_style": "professional_clear",
            "formality": "formal"
        },
        "ja": {  # Japanese
            "cultural_adaptation": True,
            "voice_style": "polite_respectful",
            "formality": "high"
        }
    }
    return configs.get(language_code, configs["es"])

# Example usage
marketing_video_localized = localize_video_content(
    "marketing_video.mp4",
    target_languages=["es", "fr", "de", "ja"],
    speaker_info={"primary_speaker": "CEO", "tone": "professional"}
)
```

### Podcast Translation

```python
def translate_podcast_episode(episode_file, episode_metadata):
    """Translate podcast episode while preserving host personality"""
    
    # Analyze episode structure
    episode_analysis = client.speech_translation.analyze_content(
        audio=episode_file,
        content_type="podcast",
        detect_segments=True  # Intro, main content, ads, outro
    )
    
    translated_episodes = {}
    
    for target_lang in episode_metadata["target_languages"]:
        # Translate main content
        main_content = client.speech_translation.translate(
            audio=episode_file,
            source_language=episode_metadata["original_language"],
            target_language=target_lang,
            
            # Podcast-specific settings
            preserve_personality=True,    # Keep host's speaking style
            conversational_tone=True,     # Maintain casual tone
            preserve_humor=True,          # Attempt to preserve jokes
            cultural_adaptation=True,     # Adapt cultural references
            
            # Technical settings
            preserve_timing=False,        # Allow natural timing in target language
            segment_based=True,           # Process in logical segments
            consistency_mode=True         # Maintain terminology consistency
        )
        
        # Generate localized intro/outro if needed
        if episode_metadata.get("localize_branding"):
            localized_intro = generate_localized_intro(
                original_intro=episode_analysis.intro_segment,
                target_language=target_lang,
                show_info=episode_metadata
            )
        
        translated_episodes[target_lang] = {
            "main_audio": main_content.audio_data,
            "transcript": main_content.translated_text,
            "intro": localized_intro if episode_metadata.get("localize_branding") else None,
            "quality_score": main_content.quality_metrics
        }
    
    return translated_episodes

def generate_localized_intro(original_intro, target_language, show_info):
    """Generate culturally appropriate intro for target market"""
    
    # Translate intro text with cultural adaptation
    intro_text = client.speech_translation.translate_text(
        text=show_info["intro_script"],
        source_language="en",
        target_language=target_language,
        style="broadcast",
        cultural_adaptation=True
    )
    
    # Generate intro with appropriate voice
    intro_audio = client.text_to_speech.create(
        text=intro_text.translated_text,
        voice_id=show_info["voices"][target_language],
        style="podcast_intro",
        energy="medium_high"
    )
    
    return intro_audio.audio_data
```

### Educational Content Translation

```python
def translate_educational_content(course_materials, target_languages):
    """Translate educational videos with terminology consistency"""
    
    # Build course-specific glossary
    course_glossary = build_educational_glossary(course_materials["subject"])
    
    translated_courses = {}
    
    for lang in target_languages:
        course_videos = {}
        
        for lesson_id, video_file in course_materials["videos"].items():
            print(f"Translating lesson {lesson_id} to {lang}...")
            
            translated_lesson = client.speech_translation.translate(
                audio=video_file,
                source_language="en",
                target_language=lang,
                
                # Educational settings
                terminology_glossary=course_glossary[lang],
                consistency_mode="strict",        # Strict term consistency
                educational_style=True,          # Clear, pedagogical delivery
                preserve_emphasis=True,          # Keep important emphasis
                
                # Technical terms handling
                preserve_technical_terms=True,   # Keep English technical terms if needed
                pronunciation_guide=True,        # Provide pronunciation for terms
                
                # Quality settings
                quality="premium",
                speaker_adaptation=True          # Adapt to educational speaking style
            )
            
            course_videos[lesson_id] = {
                "audio": translated_lesson.audio_data,
                "transcript": translated_lesson.translated_text,
                "terminology_used": translated_lesson.terminology_matches,
                "pronunciation_notes": translated_lesson.pronunciation_guide
            }
        
        translated_courses[lang] = course_videos
    
    return translated_courses

def build_educational_glossary(subject):
    """Build subject-specific terminology glossary"""
    
    # Pre-built educational glossaries
    glossaries = {
        "computer_science": {
            "es": {
                "algorithm": "algoritmo",
                "data structure": "estructura de datos",
                "machine learning": "aprendizaje automático",
                "neural network": "red neuronal"
            },
            "fr": {
                "algorithm": "algorithme", 
                "data structure": "structure de données",
                "machine learning": "apprentissage automatique",
                "neural network": "réseau de neurones"
            }
        },
        "business": {
            "es": {
                "revenue": "ingresos",
                "profit margin": "margen de beneficio",
                "market share": "cuota de mercado"
            }
        }
    }
    
    return glossaries.get(subject, {})
```

### Live Event Dubbing

```python
def setup_live_event_dubbing(event_config):
    """Set up real-time dubbing for live events"""
    
    # Create multiple language streams
    dubbing_streams = {}
    
    for target_lang in event_config["target_languages"]:
        stream = client.speech_translation.create_stream(
            source_language=event_config["source_language"],
            target_language=target_lang,
            
            # Live event settings
            latency="ultra_low",          # Minimize delay
            buffer_duration=2.0,          # Short buffer for responsiveness  
            real_time_optimization=True,  # Optimize for live processing
            
            # Quality vs speed balance
            quality="balanced",           # Balance quality and speed
            preserve_voice=True,
            emotion_preservation=True,
            
            # Live-specific features
            automatic_punctuation=True,   # Add punctuation in real-time
            noise_suppression=True,       # Handle venue noise
            speaker_adaptation=True       # Adapt to different speakers
        )
        
        dubbing_streams[target_lang] = stream
    
    return dubbing_streams

def process_live_audio(dubbing_streams, audio_chunk):
    """Process audio chunk for all target languages"""
    
    translations = {}
    
    for lang, stream in dubbing_streams.items():
        try:
            result = stream.process(audio_chunk)
            
            if result.has_translation:
                translations[lang] = {
                    "audio": result.translated_audio,
                    "text": result.translated_text,
                    "confidence": result.confidence_score,
                    "latency": result.processing_latency
                }
                
        except Exception as e:
            print(f"Error processing {lang}: {e}")
            # Implement fallback or retry logic
    
    return translations

# Example: Conference dubbing setup
conference_streams = setup_live_event_dubbing({
    "source_language": "en",
    "target_languages": ["es", "fr", "de", "zh"],
    "event_type": "business_conference"
})
```

## Configuration Options

### Translation Quality Settings

```python
response = client.speech_translation.translate(
    audio=audio_file,
    source_language="en",
    target_language="es",
    
    # Quality vs Speed
    processing_mode="quality",    # speed, balanced, quality
    quality_level="premium",      # standard, high, premium
    
    # Voice preservation
    preserve_voice=True,
    voice_similarity_threshold=0.8,  # Minimum voice similarity
    voice_adaptation_strength=0.7,   # How much to adapt voice
    
    # Translation accuracy
    translation_model="neural_v3",   # Latest model version
    context_awareness=True,          # Use context for better translation
    terminology_consistency=True,    # Maintain term consistency
    
    # Timing and synchronization
    preserve_timing=True,
    timing_flexibility=0.15,         # Allow 15% timing variation
    speed_adjustment_limit=1.2,      # Max 20% speed change
    
    # Audio processing
    noise_reduction=True,
    volume_normalization=True,
    artifact_reduction=True,
    
    # Output format
    output_format="mp3",             # mp3, wav, aac
    sample_rate=44100,
    bit_rate=192                     # kbps for MP3
)
```

### Batch Processing

```python
# Process multiple files efficiently
batch_job = client.speech_translation.batch_translate(
    files=[
        {"file": "video1.mp4", "target_languages": ["es", "fr"]},
        {"file": "video2.mp4", "target_languages": ["de", "it"]},
        {"file": "audio1.mp3", "target_languages": ["ja", "ko"]}
    ],
    source_language="en",
    
    # Batch settings
    priority="normal",               # low, normal, high, urgent
    parallel_processing=True,        # Process multiple files simultaneously
    progress_webhook="https://your-app.com/webhook/progress",
    
    # Common settings for all files
    preserve_voice=True,
    quality="high",
    cultural_adaptation=True
)

# Monitor batch progress
status = client.speech_translation.get_batch_status(batch_job.id)
print(f"Progress: {status.completed}/{status.total} files")
```

## Pricing

Speech translation pricing is based on audio duration and language pair:

### Base Pricing
- **15 credits per minute** for Tier 1 language pairs
- **20 credits per minute** for Tier 2 language pairs  
- **25 credits per minute** for Tier 3 language pairs

### Additional Features
- Voice preservation: +5 credits per minute
- Real-time processing: +10 credits per minute
- Premium quality: +8 credits per minute
- Video dubbing: +12 credits per minute

### Cost Examples

| Content | Duration | Language Pair | Features | Credits | USD Cost |
|---------|----------|---------------|----------|---------|----------|
| Audio | 2 minutes | EN→ES (Tier 1) | Basic | 30 | $0.03 |
| Video | 5 minutes | EN→ES (Tier 1) | Voice preservation + Video | 135 | $0.135 |
| Podcast | 30 minutes | EN→JA (Tier 2) | Voice preservation | 750 | $0.75 |
| Live stream | 60 minutes | EN→FR (Tier 1) | Real-time + Voice | 1800 | $1.80 |

## Best Practices

### Content Preparation

**✅ Recommended:**
- Clear, high-quality audio
- Minimal background noise
- Natural speaking pace
- Single speaker per segment when possible
- Proper pronunciation of names and terms

**❌ Avoid:**
- Overlapping speakers
- Heavy accents without context
- Technical jargon without glossary
- Very fast speech
- Poor audio quality

### Translation Optimization

```python
# Good: Provide context and terminology
contextual_translation = {
    "context": "business presentation",
    "terminology_glossary": business_terms,
    "speaker_info": "CEO, formal tone",
    "cultural_adaptation": True
}

# Better: Segment-based processing for long content
for segment in audio_segments:
    translated_segment = client.speech_translation.translate(
        audio=segment,
        **contextual_translation,
        previous_context=previous_segments_context
    )
```

## Next Steps

<Columns cols={2}>
  <Card title="Voice Cloning" icon="user-clone" href="/features/voice-cloning">
    Create custom voices for consistent dubbing across content.
  </Card>
  <Card title="Voice Changer" icon="magic-wand" href="/features/voice-changer">
    Further customize voices after translation.
  </Card>
  <Card title="API Reference" icon="code" href="/api-reference/speech-translation">
    Detailed API documentation for speech translation.
  </Card>
  <Card title="Localization Guide" icon="globe" href="/guides/content-localization">
    Best practices for content localization workflows.
  </Card>
</Columns>
