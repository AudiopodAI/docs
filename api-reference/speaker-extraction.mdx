---
title: "Speaker Extraction"
description: "Automatically identify, separate, and extract individual speakers from multi-speaker audio recordings using AudioPod AI's advanced speaker diarization and separation technology."
---

## Overview

AudioPod AI's Speaker Extraction technology uses the advanced pyannote/speaker-diarization-3.1 model to automatically identify "who spoke when" and separate individual speakers from multi-speaker audio recordings. Our state-of-the-art system provides precise speaker diarization and extraction with industry-leading accuracy.

### Key Features

- **Automatic Speaker Detection**: Identify up to 20 speakers in a recording
- **High Accuracy Separation**: 95%+ accuracy for clear audio with distinct speakers
- **Speaker Embedding**: Generate unique voice fingerprints for each speaker
- **Real-time Processing**: Live speaker separation for streaming applications
- **Voice Clustering**: Group similar voices across different recordings
- **Gender and Age Estimation**: Automatic demographic analysis
- **Emotion Detection**: Identify emotional states per speaker
- **Cross-Session Recognition**: Recognize speakers across multiple recordings

## Quick Start

### Basic Speaker Extraction

<Tabs>
  <Tab title="Python">
    ```python
    from audiopod import AudioPod

    client = AudioPod(api_key="your_api_key")

    # Extract speakers from multi-speaker audio
    with open("meeting_recording.mp3", "rb") as audio_file:
        response = client.speaker_extraction.extract(
            audio=audio_file,
            max_speakers=5,
            min_speaker_duration=2.0,  # Minimum 2 seconds per speaker
            include_metadata=True
        )

    # Save individual speaker tracks
    for speaker_id, speaker_data in response.speakers.items():
        with open(f"speaker_{speaker_id}.wav", "wb") as f:
            f.write(speaker_data.audio)

        print(f"Speaker {speaker_id}:")
        print(f"  Gender: {speaker_data.demographics.gender}")
        print(f"  Age: {speaker_data.demographics.age_range}")
        print(f"  Speaking time: {speaker_data.speaking_duration}s")
        print(f"  Confidence: {speaker_data.confidence}")

    print(f"Extracted {len(response.speakers)} speakers")
    print(f"Total duration: {response.duration}s")
    ```

  </Tab>
  <Tab title="Node.js">
    ```javascript
    import { AudioPod } from 'audiopod-js';
    import fs from 'fs';

    const client = new AudioPod({ apiKey: 'your_api_key' });

    const audioFile = fs.readFileSync('meeting_recording.mp3');

    const response = await client.speakerExtraction.extract({
      audio: audioFile,
      maxSpeakers: 5,
      minSpeakerDuration: 2.0,
      includeMetadata: true
    });

    // Save individual speaker tracks
    for (const [speakerId, speakerData] of Object.entries(response.speakers)) {
      fs.writeFileSync(`speaker_${speakerId}.wav`, speakerData.audio);

      console.log(`Speaker ${speakerId}:`);
      console.log(`  Gender: ${speakerData.demographics.gender}`);
      console.log(`  Age: ${speakerData.demographics.ageRange}`);
      console.log(`  Speaking time: ${speakerData.speakingDuration}s`);
      console.log(`  Confidence: ${speakerData.confidence}`);
    }

    console.log(`Extracted ${Object.keys(response.speakers).length} speakers`);
    console.log(`Total duration: ${response.duration}s`);
    ```

  </Tab>
  <Tab title="cURL">
    ```bash
    curl -X POST "https://api.audiopod.ai/api/v1/speaker-extraction" \
      -H "Authorization: Bearer your_api_key" \
      -F "audio=@meeting_recording.mp3" \
      -F "max_speakers=5" \
      -F "min_speaker_duration=2.0" \
      -F "include_metadata=true" \
      -o speakers.zip
    ```
  </Tab>
</Tabs>

### Response Format

```json
{
  "speakers": {
    "speaker_1": {
      "audio": "base64_encoded_audio",
      "segments": [
        {
          "start": 0.0,
          "end": 15.3,
          "text": "Hello everyone, welcome to the meeting"
        },
        {
          "start": 45.2,
          "end": 62.1,
          "text": "Let's start with the quarterly review"
        }
      ],
      "speaking_duration": 127.5,
      "demographics": {
        "gender": "female",
        "age_range": "25-35",
        "accent": "american_english"
      },
      "voice_characteristics": {
        "pitch_range": "180-280 Hz",
        "speaking_rate": "normal",
        "emotion_profile": ["confident", "professional"]
      },
      "confidence": 0.94
    },
    "speaker_2": {
      "audio": "base64_encoded_audio",
      "segments": [
        {
          "start": 16.1,
          "end": 28.7,
          "text": "Thank you Sarah, I have the numbers ready"
        },
        {
          "start": 63.0,
          "end": 89.4,
          "text": "Our revenue increased by 15% this quarter"
        }
      ],
      "speaking_duration": 98.3,
      "demographics": {
        "gender": "male",
        "age_range": "35-45",
        "accent": "british_english"
      },
      "confidence": 0.89
    }
  },
  "duration": 300.0,
  "processing_info": {
    "model_used": "speaker_v3",
    "detection_accuracy": 0.92,
    "separation_quality": 0.88,
    "processing_time": 45.2
  }
}
```

## Advanced Speaker Analysis

### Speaker Identification with Labels

Provide known speaker information for better accuracy:

```python
response = client.speaker_extraction.extract(
    audio=audio_file,

    # Known speakers
    speaker_labels=["Sarah Johnson", "Mike Chen", "Dr. Smith"],
    speaker_hints=[
        {"name": "Sarah Johnson", "gender": "female", "role": "moderator"},
        {"name": "Mike Chen", "gender": "male", "role": "analyst"},
        {"name": "Dr. Smith", "gender": "male", "role": "expert"}
    ],

    # Reference audio samples (if available)
    speaker_references={
        "Sarah Johnson": "sarah_voice_sample.wav",
        "Mike Chen": "mike_voice_sample.wav"
    },

    # Enhanced processing
    use_speaker_embeddings=True,   # Generate voice fingerprints
    cross_talk_handling=True,      # Handle overlapping speech
    background_noise_reduction=True
)

# Access labeled speakers
for speaker_name, speaker_data in response.labeled_speakers.items():
    print(f"{speaker_name}: {speaker_data.speaking_duration}s")
    print(f"Recognition confidence: {speaker_data.identification_confidence}")
```

### Emotion and Sentiment Analysis

Analyze emotional state of each speaker:

```python
response = client.speaker_extraction.extract(
    audio=audio_file,
    max_speakers=4,

    # Emotion analysis
    include_emotion_analysis=True,
    emotion_granularity="detailed",  # basic, detailed, comprehensive
    sentiment_analysis=True,
    stress_detection=True
)

# Analyze speaker emotions
for speaker_id, speaker_data in response.speakers.items():
    emotions = speaker_data.emotion_analysis

    print(f"Speaker {speaker_id} emotional profile:")
    print(f"  Dominant emotion: {emotions.dominant_emotion}")
    print(f"  Emotional intensity: {emotions.intensity}")
    print(f"  Sentiment: {emotions.sentiment}")
    print(f"  Stress level: {emotions.stress_indicators.level}")

    # Segment-level emotions
    for segment in speaker_data.segments:
        if segment.emotion:
            print(f"  {segment.start}s-{segment.end}s: {segment.emotion}")
```

### Speaker Clustering Across Sessions

Group speakers across multiple recordings:

```python
# Initialize speaker clustering session
clustering_session = client.speaker_extraction.create_clustering_session(
    session_name="weekly_meetings",
    clustering_sensitivity=0.8,    # How similar voices need to be
    max_speakers_per_session=10
)

# Process multiple meeting recordings
meeting_files = ["meeting_1.mp3", "meeting_2.mp3", "meeting_3.mp3"]

for meeting_file in meeting_files:
    with open(meeting_file, "rb") as audio:
        session_result = clustering_session.add_recording(
            audio=audio,
            recording_metadata={
                "date": "2024-01-15",
                "meeting_type": "weekly_standup",
                "expected_participants": ["Alice", "Bob", "Charlie"]
            }
        )

    print(f"Processed {meeting_file}:")
    print(f"  New speakers detected: {session_result.new_speakers}")
    print(f"  Recognized speakers: {session_result.recognized_speakers}")

# Get cross-session speaker analysis
speaker_patterns = clustering_session.get_speaker_patterns()

for speaker_id, pattern in speaker_patterns.items():
    print(f"Speaker {speaker_id}:")
    print(f"  Appeared in {pattern.session_count} meetings")
    print(f"  Total speaking time: {pattern.total_speaking_time}s")
    print(f"  Average participation: {pattern.participation_percentage}%")
    print(f"  Speaking pattern: {pattern.behavior_analysis}")
```

## Real-time Speaker Extraction

For live meetings and streaming applications:

```python
# Initialize real-time speaker extraction
live_extractor = client.speaker_extraction.create_stream(
    max_speakers=6,
    latency="low",                 # low, medium, high
    buffer_duration=5.0,           # 5-second processing buffer
    real_time_labeling=True,       # Assign speaker labels in real-time
    overlap_handling=True          # Handle overlapping speech
)

# Process live audio stream
def process_live_meeting():
    current_speakers = {}

    while meeting_active:
        # Get audio chunk from microphone array
        audio_chunk = get_audio_input()

        # Extract speakers in real-time
        result = live_extractor.process(audio_chunk)

        # Handle speaker changes
        if result.speaker_change_detected:
            print(f"Speaker change: {result.previous_speaker} → {result.current_speaker}")

        # Route audio to speaker-specific outputs
        for speaker_id, speaker_audio in result.current_speakers.items():
            if speaker_id not in current_speakers:
                # New speaker detected
                print(f"New speaker detected: {speaker_id}")
                current_speakers[speaker_id] = create_speaker_profile(speaker_id)

            # Update speaker's audio stream
            current_speakers[speaker_id].update_audio(speaker_audio)

        # Generate real-time transcription per speaker
        for speaker_id, transcription in result.transcriptions.items():
            if transcription.is_final:
                print(f"{speaker_id}: {transcription.text}")

# Clean up
live_extractor.close()
```

## Use Cases & Examples

### Meeting Analytics and Transcription

```python
def analyze_meeting_dynamics(meeting_audio, participant_list=None):
    """Comprehensive meeting analysis with speaker insights"""

    # Extract speakers with detailed analysis
    extraction = client.speaker_extraction.extract(
        audio=meeting_audio,
        max_speakers=len(participant_list) if participant_list else 10,
        speaker_labels=participant_list,

        # Comprehensive analysis
        include_emotion_analysis=True,
        include_interaction_analysis=True,
        include_participation_metrics=True,
        segment_level_analysis=True
    )

    # Generate meeting analytics
    meeting_analytics = {
        "participants": {},
        "interaction_patterns": {},
        "meeting_metrics": {},
        "recommendations": []
    }

    total_speaking_time = sum(
        speaker.speaking_duration for speaker in extraction.speakers.values()
    )

    # Analyze each participant
    for speaker_id, speaker_data in extraction.speakers.items():
        speaker_name = speaker_data.label or f"Speaker {speaker_id}"

        participation_metrics = {
            "speaking_time": speaker_data.speaking_duration,
            "participation_percentage": (speaker_data.speaking_duration / total_speaking_time) * 100,
            "number_of_turns": len(speaker_data.segments),
            "average_turn_length": speaker_data.speaking_duration / len(speaker_data.segments),
            "interruptions": speaker_data.interaction_data.interruption_count,
            "questions_asked": count_questions(speaker_data.segments),
            "dominant_emotions": speaker_data.emotion_analysis.emotion_distribution
        }

        meeting_analytics["participants"][speaker_name] = participation_metrics

    # Analyze interaction patterns
    meeting_analytics["interaction_patterns"] = analyze_speaking_patterns(extraction)

    # Generate recommendations
    meeting_analytics["recommendations"] = generate_meeting_recommendations(
        meeting_analytics["participants"],
        meeting_analytics["interaction_patterns"]
    )

    return meeting_analytics

def generate_meeting_recommendations(participants, patterns):
    """Generate actionable meeting insights"""

    recommendations = []

    # Check for participation balance
    participation_scores = [p["participation_percentage"] for p in participants.values()]
    participation_std = np.std(participation_scores)

    if participation_std > 20:  # High variation in participation
        dominant_speaker = max(participants.items(), key=lambda x: x[1]["participation_percentage"])
        quiet_speakers = [name for name, data in participants.items()
                         if data["participation_percentage"] < 10]

        recommendations.append({
            "type": "participation_balance",
            "priority": "medium",
            "message": f"{dominant_speaker[0]} dominated the conversation. "
                      f"Consider encouraging input from {', '.join(quiet_speakers)}."
        })

    # Check for excessive interruptions
    high_interrupters = [name for name, data in participants.items()
                        if data["interruptions"] > 5]

    if high_interrupters:
        recommendations.append({
            "type": "meeting_flow",
            "priority": "high",
            "message": f"High interruption rate detected from {', '.join(high_interrupters)}. "
                      "Consider implementing speaking order or hand-raising."
        })

    return recommendations
```

### Podcast Production and Editing

```python
def process_podcast_recording(podcast_audio, host_info, guest_info=None):
    """Process podcast recording for editing and production"""

    # Set up known speakers
    speakers_config = [host_info]
    if guest_info:
        if isinstance(guest_info, list):
            speakers_config.extend(guest_info)
        else:
            speakers_config.append(guest_info)

    # Extract speakers with podcast-specific settings
    extraction = client.speaker_extraction.extract(
        audio=podcast_audio,
        speaker_labels=[s["name"] for s in speakers_config],
        speaker_hints=speakers_config,

        # Podcast-specific settings
        optimize_for_conversation=True,
        remove_background_noise=True,
        normalize_volume_levels=True,
        detect_laughter_applause=True,

        # Quality settings for editing
        quality="premium",
        preserve_audio_quality=True,
        include_silence_detection=True
    )

    # Process for podcast editing
    edited_tracks = {}

    for speaker_id, speaker_data in extraction.speakers.items():
        speaker_name = speaker_data.label

        # Clean up speaker's audio
        cleaned_audio = client.audio_processing.enhance(
            audio=speaker_data.audio,
            remove_background_noise=True,
            reduce_mouth_sounds=True,
            normalize_volume=True,
            add_compression=True,  # Podcast-appropriate compression
            eq_for_voice=True
        )

        # Generate edit points
        edit_points = identify_edit_points(speaker_data.segments)

        edited_tracks[speaker_name] = {
            "clean_audio": cleaned_audio.audio_data,
            "original_segments": speaker_data.segments,
            "suggested_edits": edit_points,
            "speaking_stats": {
                "total_time": speaker_data.speaking_duration,
                "word_count": estimate_word_count(speaker_data.segments),
                "filler_words": count_filler_words(speaker_data.segments),
                "energy_level": speaker_data.voice_characteristics.energy_level
            }
        }

    # Generate podcast production report
    production_report = {
        "total_duration": extraction.duration,
        "speaker_tracks": edited_tracks,
        "quality_metrics": extraction.processing_info,
        "editing_suggestions": generate_editing_suggestions(extraction),
        "export_ready": True
    }

    return production_report

def identify_edit_points(speaker_segments):
    """Identify potential edit points for smooth podcast flow"""

    edit_points = []

    for i, segment in enumerate(speaker_segments):
        # Detect long pauses that could be trimmed
        if hasattr(segment, 'silence_duration') and segment.silence_duration > 2.0:
            edit_points.append({
                "type": "trim_silence",
                "timestamp": segment.end,
                "duration": segment.silence_duration,
                "suggestion": f"Trim {segment.silence_duration:.1f}s silence"
            })

        # Detect filler words
        if detect_filler_words(segment.text):
            edit_points.append({
                "type": "remove_filler",
                "timestamp": segment.start,
                "text": segment.text,
                "suggestion": "Consider removing filler words"
            })

        # Detect potential cut points between topics
        if i < len(speaker_segments) - 1:
            next_segment = speaker_segments[i + 1]
            time_gap = next_segment.start - segment.end

            if time_gap > 5.0:  # Long gap might indicate topic change
                edit_points.append({
                    "type": "topic_transition",
                    "timestamp": segment.end,
                    "suggestion": "Potential topic transition point"
                })

    return edit_points
```

### Call Center Quality Assurance

```python
def analyze_customer_service_call(call_audio, agent_info=None):
    """Analyze customer service calls for quality assurance"""

    # Extract customer and agent
    extraction = client.speaker_extraction.extract(
        audio=call_audio,
        max_speakers=2,  # Typically customer + agent
        speaker_labels=["Agent", "Customer"] if not agent_info else [agent_info["name"], "Customer"],

        # Call center specific analysis
        include_emotion_analysis=True,
        include_sentiment_analysis=True,
        detect_keywords=True,
        keyword_categories=["complaint", "compliment", "frustration", "satisfaction"],

        # Quality metrics
        include_speech_quality_metrics=True,
        detect_interruptions=True,
        measure_response_times=True
    )

    # Identify agent and customer
    agent_speaker = None
    customer_speaker = None

    for speaker_id, speaker_data in extraction.speakers.items():
        if speaker_data.label == "Agent" or (agent_info and speaker_data.label == agent_info["name"]):
            agent_speaker = speaker_data
        else:
            customer_speaker = speaker_data

    # Analyze call quality
    call_analysis = {
        "call_duration": extraction.duration,
        "agent_performance": analyze_agent_performance(agent_speaker),
        "customer_experience": analyze_customer_experience(customer_speaker),
        "interaction_quality": analyze_interaction_quality(extraction),
        "compliance_check": check_compliance(extraction),
        "quality_score": 0.0  # Will be calculated
    }

    # Calculate overall quality score
    call_analysis["quality_score"] = calculate_call_quality_score(call_analysis)

    return call_analysis

def analyze_agent_performance(agent_data):
    """Analyze agent performance metrics"""

    performance_metrics = {
        "professionalism_score": 0.0,
        "empathy_indicators": [],
        "resolution_keywords": [],
        "response_time": 0.0,
        "speaking_pace": "normal",
        "emotional_tone": "neutral"
    }

    if agent_data.emotion_analysis:
        # Analyze emotional appropriateness
        emotions = agent_data.emotion_analysis.emotion_distribution
        if emotions.get("professional", 0) > 0.6:
            performance_metrics["professionalism_score"] = 0.9
        elif emotions.get("empathetic", 0) > 0.4:
            performance_metrics["professionalism_score"] = 0.8

    # Check for empathy indicators in speech
    empathy_phrases = ["I understand", "I'm sorry", "Let me help", "I can imagine"]
    for segment in agent_data.segments:
        for phrase in empathy_phrases:
            if phrase.lower() in segment.text.lower():
                performance_metrics["empathy_indicators"].append({
                    "phrase": phrase,
                    "timestamp": segment.start
                })

    # Analyze speaking characteristics
    voice_chars = agent_data.voice_characteristics
    if voice_chars.speaking_rate < 0.8:
        performance_metrics["speaking_pace"] = "slow"
    elif voice_chars.speaking_rate > 1.2:
        performance_metrics["speaking_pace"] = "fast"

    return performance_metrics

def analyze_customer_experience(customer_data):
    """Analyze customer experience and satisfaction"""

    experience_metrics = {
        "satisfaction_indicators": [],
        "frustration_level": 0.0,
        "resolution_confidence": 0.0,
        "emotional_journey": []
    }

    if customer_data.emotion_analysis:
        # Track emotional journey throughout call
        for segment in customer_data.segments:
            if hasattr(segment, 'emotion'):
                experience_metrics["emotional_journey"].append({
                    "timestamp": segment.start,
                    "emotion": segment.emotion,
                    "intensity": segment.emotion_intensity
                })

        # Calculate frustration level
        frustration_emotions = ["angry", "frustrated", "annoyed"]
        frustration_score = sum(
            customer_data.emotion_analysis.emotion_distribution.get(emotion, 0)
            for emotion in frustration_emotions
        )
        experience_metrics["frustration_level"] = frustration_score

    # Look for satisfaction indicators
    satisfaction_phrases = ["thank you", "perfect", "great", "solved", "helpful"]
    for segment in customer_data.segments:
        for phrase in satisfaction_phrases:
            if phrase.lower() in segment.text.lower():
                experience_metrics["satisfaction_indicators"].append({
                    "phrase": phrase,
                    "timestamp": segment.start
                })

    return experience_metrics
```

### Interview Analysis and Recruitment

```python
def analyze_job_interview(interview_audio, interviewer_info, candidate_info):
    """Analyze job interview for recruitment insights"""

    # Extract interviewer and candidate
    extraction = client.speaker_extraction.extract(
        audio=interview_audio,
        max_speakers=2,
        speaker_labels=[interviewer_info["name"], candidate_info["name"]],
        speaker_hints=[interviewer_info, candidate_info],

        # Interview-specific analysis
        include_emotion_analysis=True,
        include_confidence_analysis=True,
        include_speech_patterns=True,
        detect_keywords=True,
        keyword_categories=["technical_skills", "soft_skills", "experience"]
    )

    # Identify speakers
    interviewer = None
    candidate = None

    for speaker_id, speaker_data in extraction.speakers.items():
        if speaker_data.label == interviewer_info["name"]:
            interviewer = speaker_data
        else:
            candidate = speaker_data

    # Analyze interview dynamics
    interview_analysis = {
        "interview_duration": extraction.duration,
        "candidate_performance": analyze_candidate_performance(candidate),
        "interviewer_style": analyze_interviewer_style(interviewer),
        "interaction_dynamics": analyze_interview_dynamics(extraction),
        "technical_assessment": extract_technical_content(candidate),
        "soft_skills_assessment": assess_soft_skills(candidate),
        "overall_recommendation": generate_interview_recommendation(candidate, interviewer)
    }

    return interview_analysis

def analyze_candidate_performance(candidate_data):
    """Analyze candidate's interview performance"""

    performance = {
        "confidence_level": 0.0,
        "communication_clarity": 0.0,
        "technical_depth": 0.0,
        "enthusiasm_score": 0.0,
        "response_quality": [],
        "red_flags": [],
        "strengths": []
    }

    # Analyze confidence through voice characteristics
    if candidate_data.voice_characteristics:
        voice_chars = candidate_data.voice_characteristics

        # Confidence indicators
        if voice_chars.pitch_stability > 0.7 and voice_chars.volume_consistency > 0.8:
            performance["confidence_level"] = 0.8
        elif voice_chars.pitch_stability < 0.5:
            performance["red_flags"].append("Voice instability suggesting nervousness")

    # Analyze emotional indicators
    if candidate_data.emotion_analysis:
        emotions = candidate_data.emotion_analysis.emotion_distribution

        if emotions.get("confident", 0) > 0.5:
            performance["strengths"].append("Demonstrates confidence")
        if emotions.get("enthusiastic", 0) > 0.4:
            performance["enthusiasm_score"] = 0.8
        if emotions.get("nervous", 0) > 0.6:
            performance["red_flags"].append("High nervousness throughout interview")

    # Analyze response patterns
    for segment in candidate_data.segments:
        response_analysis = analyze_interview_response(segment)
        performance["response_quality"].append(response_analysis)

    return performance

def analyze_interview_response(segment):
    """Analyze individual interview responses"""

    response_metrics = {
        "timestamp": segment.start,
        "duration": segment.end - segment.start,
        "word_count": len(segment.text.split()),
        "complexity_score": 0.0,
        "clarity_score": 0.0,
        "content_quality": "average"
    }

    # Analyze response length appropriateness
    if response_metrics["duration"] < 10:
        response_metrics["content_quality"] = "too_brief"
    elif response_metrics["duration"] > 120:
        response_metrics["content_quality"] = "too_lengthy"
    else:
        response_metrics["content_quality"] = "appropriate"

    # Analyze vocabulary complexity
    complex_words = count_complex_words(segment.text)
    response_metrics["complexity_score"] = complex_words / response_metrics["word_count"]

    # Analyze clarity (fewer filler words = higher clarity)
    filler_count = count_filler_words(segment.text)
    response_metrics["clarity_score"] = max(0, 1 - (filler_count / response_metrics["word_count"]))

    return response_metrics
```

## Configuration Options

### Processing Parameters

```python
response = client.speaker_extraction.extract(
    audio=audio_file,

    # Speaker detection
    max_speakers=10,               # Maximum speakers to detect
    min_speaker_duration=1.0,      # Minimum speaking time (seconds)
    speaker_change_sensitivity=0.7, # Sensitivity to speaker changes

    # Audio processing
    noise_reduction=True,          # Reduce background noise
    volume_normalization=True,     # Normalize speaker volumes
    overlap_handling="smart",      # none, basic, smart, advanced

    # Analysis options
    include_demographics=True,     # Gender, age estimation
    include_emotions=True,         # Emotional analysis
    include_voice_characteristics=True, # Pitch, rate, etc.
    include_transcription=True,    # Speech-to-text

    # Quality settings
    quality="high",                # standard, high, premium
    model="speaker_v3",            # Latest model version
    processing_priority="balanced", # speed, balanced, quality

    # Output format
    output_format="wav",           # wav, mp3, flac
    separate_audio_tracks=True,    # Individual audio files per speaker
    include_timestamps=True        # Precise timing information
)
```

### Batch Processing

Process multiple recordings with speaker consistency:

```python
# Batch process with speaker tracking
batch_job = client.speaker_extraction.batch_extract(
    files=[
        {"file": "meeting_day1.mp3", "expected_speakers": ["Alice", "Bob", "Charlie"]},
        {"file": "meeting_day2.mp3", "expected_speakers": ["Alice", "Bob", "David"]},
        {"file": "meeting_day3.mp3", "expected_speakers": ["Alice", "Charlie", "Eve"]}
    ],

    # Cross-session speaker tracking
    enable_speaker_clustering=True,
    clustering_threshold=0.85,     # Speaker similarity threshold
    speaker_consistency_check=True,

    # Processing settings
    quality="high",
    include_comprehensive_analysis=True,
    webhook_url="https://your-app.com/webhook/speaker-extraction"
)

# Monitor progress
status = client.speaker_extraction.get_batch_status(batch_job.id)
```

## Pricing

Speaker extraction pricing is based on audio duration and analysis features:

### Base Pricing

- **8 credits per minute** for basic speaker extraction
- **12 credits per minute** with demographic analysis
- **16 credits per minute** with comprehensive analysis (emotions, characteristics)

### Additional Features

- Real-time processing: +5 credits per minute
- Speaker clustering: +3 credits per minute
- Premium quality: +4 credits per minute

### Cost Examples

| Audio Length | Features             | Credits | USD Cost |
| ------------ | -------------------- | ------- | -------- |
| 5 minutes    | Basic extraction     | 40      | $0.04    |
| 30 minutes   | Full analysis        | 480     | $0.48    |
| 2 hours      | Basic + real-time    | 1,560   | $1.56    |
| 10 minutes   | Premium + clustering | 210     | $0.21    |

## Best Practices

### Audio Quality Guidelines

**✅ Recommended:**

- Clear audio with minimal background noise
- Speakers at consistent distances from microphone
- Minimal overlapping speech
- Good signal-to-noise ratio
- Consistent volume levels

**❌ Avoid:**

- Heavy background music or noise
- Multiple speakers talking simultaneously
- Very quiet or distant speakers
- Poor quality recordings (phone calls, etc.)
- Heavily compressed audio

### Optimization Tips

```python
# Good: Provide speaker hints for better accuracy
response = client.speaker_extraction.extract(
    audio=audio_file,
    speaker_hints=[
        {"gender": "female", "age_range": "25-35", "role": "moderator"},
        {"gender": "male", "age_range": "40-50", "role": "participant"}
    ],
    max_speakers=4  # Reasonable limit
)

# Better: Use reference samples when available
response = client.speaker_extraction.extract(
    audio=audio_file,
    speaker_references={
        "John": "john_voice_sample.wav",
        "Sarah": "sarah_voice_sample.wav"
    }
)
```

## Next Steps

<Columns cols={2}>
  <Card title="Speech-to-Text" icon="waveform" href="/features/speech-to-text">
    Combine speaker extraction with transcription.
  </Card>
  <Card title="Voice Cloning" icon="user-clone" href="/features/voice-cloning">
    Clone voices from extracted speaker audio.
  </Card>
  <Card
    title="API Reference"
    icon="code"
    href="/api-reference/speaker-extraction"
  >
    Detailed API documentation for speaker extraction.
  </Card>
  <Card
    title="Meeting Analytics"
    icon="chart-line"
    href="/guides/meeting-analytics"
  >
    Build comprehensive meeting analysis solutions.
  </Card>
</Columns>
